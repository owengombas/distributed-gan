\chapter{Experimental setup}
\section{Flower.ai first draft}
Flower.ai\footnote{\url{https://flower.ai/}} is a Python library that allows for the distributed training of machine learning models. It is designed to be framework-agnostic and can be used with any machine learning framework that supports the Python programming language, such as TensorFlow, PyTorch, and scikit-learn. Flower.ai provides a simple and intuitive API for distributed training, allowing users to easily scale their machine learning models across multiple devices and machines. However, in its current version Flower.ai enforces developpers to follow a strict pipeline for the training process. While implementing the MD-GAN, we found that the Flower.ai library was not flexible enough to accommodate the specific requirements of the MD-GAN architecture. We therefore decided to implement it using PyTorch's distributed package, which provides more control over the training process and allows for a more customized implementation.

\section{PyTorch distributed (over TCP)}
PyTorch's distributed package offers crucial capabilities for sending and receiving tensors across various processes. We selected the TCP backend within this framework to enable robust network communication between machines. This setup allows us to efficiently send feedback to the central server and distribute generated images to the workers, facilitating effective and collaborative training of the GAN across multiple nodes. However we can't send dictionaries or complex objects, which are essential for the MD-GAN architecture, since we have to exchange the state of discriminators to avoid overfitting. We therefore decided to use TensorDict\footnote{\url{https://github.com/pytorch/tensordict}}, a module integrated into PyTorch, that turns dictionaries into tensors and vice versa. It is also capable of sending and receiving these tensors across different processes using the same TCP backend, so it fits perfectly with our requirements.

\subsection{Backend choice}
In developing our federated model, selecting an effective communication backend was crucial to handle data exchanges between different nodes efficiently. 

Based on the distributed communication package\footnote{\url{https://pytorch.org/docs/stable/distributed.html}}, we initially considered using the NCCL backend because of its high performance, it uses GPUs to accelerate data communication.

However NCCL do not support having multiple node running on one GPU, referring to the figure \ref{fig:architecture} it goes against our architecture. To take advantage of the whole computational power of the instances we have we run multiple workers on a single machine. Therefore we choosed to use the GLOO backend, offering us more flexibility on how many workers we instanciate on a Compute Engine. However in the case where we have a single worker on every instance, thanks to our novel swapping strategy it is possible to use NCCL and therefore boost the performances. Although GLOO relies on CPUs for message handling, which introduces some latency as messages are transferred to GPU for processing. This trade-off is justified by the flexibility it offers.

\section{Metrics used}
To evaluate the performance and quality of the data generated by our distributed GAN, we utilized TorchMetrics\footnote{\url{https://lightning.ai/docs/torchmetrics/stable/}}, a library developed by LightningAI. This package includes over 100 implemented PyTorch metrics, significantly simplifying their integration into our model. As discussed previously, we employed two key metrics: the Frechet Inception Distance (FID) and the Inception Score (IS).

\section{Baseline comparison}
To evaluate the performance of our distributed MD-GAN implementation, we compared it to a baseline centralized implementation. The baseline implementation uses a single machine to train the GAN on the entire dataset, without any distribution of data or computation. This setup serves as a reference point for evaluating the benefits of distributed training and allows us to measure the impact of distributing the training process across multiple machines. We used the same hyperparameters and training settings for both the distributed and centralized implementations to ensure a fair comparison. It is important to note that the centralized implementation is requires to be run on a single machine, and therefore that data must be owned by the machine running the training process, which is not the case for the distributed implementation.

\section{Datasets used}
To evaluate our distributed Multi-Discriminator GAN implementation, we used three datasets: MNIST, CelebA and CIFAR-10.

\begin{itemize}
    \item The MNIST dataset consists of 60,000 grayscale images of handwritten digits with 10 classes (0-9). Each image is 28x28 pixels in size, summing up to 784 input parameters, it is a relatively small and simple dataset.
    \item CelebA consists of 202'599 celebrity faces image, each of size 178x218 and 3 channels (RGB) which sums up to 116'412 input parameters, to reduce the input size of our model we resize every image to 64x64 and preserve the 3 channels, ending up with 12'288 input parameters instead.
    \item The CIFAR-10 dataset, consists of 60,000 color images with 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck).
\end{itemize}

Even if CelebA contains more input parameters it is more conceptually simple than CIFAR-10, due to that fact that every image consists of single human face, the angle could vary but the subject is still the same. In the other hand CIFAR-10 contains very different classes, making CIFAR-10 the most complex data distribution to learn because, for instance, an image of a bird is very "different" from an image containing a truck.

We believe that these three datasets are a great choice to accurately evaluate our implementation, they are widely used in the literature and cover various type of classes and use cases, especially in the computer vision field and generative models.
% By using these three datasets, we can evaluate the performance of our distributed MD-GAN implementation on both simple and complex data distributions, providing a comprehensive assessment of its effectiveness across different scenarios. We choosed these datasets because they were the ones used in the original MD-GAN paper, so we can compare our results with the ones obtained by the authors.

\section{Local setup}
It is possible to run the distributed MD-GAN implementation on a single machine, because every process is a separate Python process. This setup also enables us to run multiple workers on the same machine, a script called \code{run-with-server.sh} is provided to facilitate starting the server and workers. This local setup is useful for testing and debugging the implementation before deploying it to a networked environment. It also allows us to evaluate the performance of the distributed MD-GAN on a small scale, providing insights into how the system behaves under different conditions. We used this setup to test the implementation on the MNIST, CelebA and CIFAR-10 datasets, as well as to verify the correctness of the communication between the server and workers.

Every run will collect statistics and log them in JSON format in the \code{logs} folder. These files can be interpreted using the Juypter Notebook \code{plot-logs.ipynb} which will generate all the plots related to the last run.

\section{Models}
For every dataset we want to consider in our experiment we add to define a generator model, discriminator model, a method which load the dataset and set some global properties of the dataset that can be shared across our implementation. For that we created an application architecture allowing us for being flexible when it comes to add new dataset to our application. The folder \code{datasets} should contains one file for every dataset, every file has to implement three classes and two constants: 
\begin{itemize}
    \item \code{Partitioner}: this class has the role to load the dataset and allow for performing some methods on that dataset, such as partitioning or selecting a subset of data.
    \item \code{Discriminator}: this class must define the architecture of the discriminator for the dataset.
    \item \code{Generator}: as for the discriminator, this class must implement the generator for the dataset.
    \item \code{SHAPE}: this is a global constant the will allow the server to generate the fake batches the workers will receive, it should be a tuple of three integer (\code{Tuple[int, int, int]}) respectively the number of channels, the width in pixels, the height in pixels.
    \item \code{Z\_DIM}: the dimension of the input noise the generator should receive to generate a new image from.
\end{itemize}

We defined three pre-exising files to be able to evaluate on MNIST, CIFAR-10 and CelebA. We had to find good model architecture adapted for every dataset, we opted for a DCGAN\footnote{A DCGAN is a GAN that explicitly uses convolutional layers \url{https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html}} for CIFAR-10 and CelebA because for complicated images a CNN is expected to perform better than a Multi-Layer Perceptron since it can learn many optimal filters to convolve on the input image for a certain task, and therefore account to local connectivity of subset of pixels from the image since the filters are apply by performing a convolution.

Also a CNN can be viewed as a special type of feed-forward neural network where the weights are constrained, unlike a feed-forward neural network, the units in the hidden layers of a CNN are not fully connected to the input units. Instead, each unit in the hidden layer is connected to a small region of the input. The weights are shared across the input space because the same filter is applied to each input location. In a feed-forward neural network, each input is connected to each hidden unit with a different weight that are not shared and are learned independently (fully connected) \cite{james2023introduction}. However the CNN and pooling layers are more computationally expensive so for small and simple dataset using a Multi-Layer Perceptron isn't a bad choice.

\figureWithCaption{cnn_fnn}{Illustration of a FNN and CNN weight sharing from \cite{Winter2018}}{10cm}

Therefore, for MNIST we choosed to implement a MLP due to its simplicity. Each of the original images online contains 1 channel compared to the other two datasets and it is also the smallest one with a size of 28x28.

The specific architecture of our models are specifed in the appendix \ref{appendix:models}.

\section{Launch scripts}
To easily start our implementation we created two scripts, \code{run-standalone.sh} and \code{run-distributed.sh}. Each of them allow us for starting our experiments in different settings. They of them define their arguments in the first lines of the script. For the arguments that are shared by both of them, we have one additional file named \code{shared-args.sh} which define the arguments shared across our different experiment settings, ensuring that they all run with the same parameters. The details of every argument are given in the appendix \ref{appendix:args}

\section{Google Cloud setup}
We received 200\$ dollars worth of Google Cloud credits by Prof. Dr. Lydia Chen and the teaching assistant Abel Malan in the context of Distributed Deep Learning class at University of Neuch√¢tel. We used these credits to launch our experiments on two Google Cloud Compute Engine instances, the three of them is equipped with a NVIDIA T4 TPU, 15GB of RAM and 4 VCPU. Therefore, we can then use the \code{cuda} device achieve short training time. These two instances communicate between each other through a Virtual Private Cloud, this can be viewed as a local private network, and both of them are on the same sub-network, allowing for better security by avoiding to communicate through internet. Although it is important to notice that even if they are both virtual on the same sub-network, they are in two different datacenters location, one is in central-europe and one in west-europe, this is an important fact to consider when analyzing our results.
\figureWithCaption{architecture}{The network architecture of our application}{\textwidth}
